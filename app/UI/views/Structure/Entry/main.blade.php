<div>
    <div>
    Finding good training hyperparameters for new LLMs is always difficult and time-consuming. With Zephyr Gemma 7B, Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used a combination of distilled supervised fine-tuning and DPO similar to what they did for their original Zephyr based on Mistral 7B. However, training Gemma with DPO on consumer hardware is challenging due to its memory consumption.

    In this article, I first review the recipe used by Hugging Face to train Zephyr Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing various optimizations for fast and memory-efficient training. The method presented in this article has a peak memory consumption of 19 GB of VRAM and a total training time of only 8 hours. In other words, DPO training for Gemma is possible on consumer hardware.

    A Closer Look at Zephyr Gemma
    Supervised Fine-tuning (SFT)
    DPO must use for reference a model trained with supervised fine-tuning (SFT) on an instruction dataset. Hugging Face also released this SFT model:

    HuggingFaceH4/zephyr-7b-gemma-sft-v0.1
    For SFT, they used deita-10k which is a small instruction dataset of 9.5k examples:

    HuggingFaceH4/deita-10k-v0-sft (MIT license)
    A wide variety of LLMs have generated all the examples in this dataset (GPT-4, GPT-3.5, Claude, Vicuna, Llama 2, Mistral 7B, Zephyr, etc.). For SFT training, they used a special data format that we will also use.

    Hugging Face used the hyperparameters referenced in this configuration file from their alignment handbook. They didn’t use LoRA or quantization. It means that they probably used many A100/H100 GPUs for training Zephyr Gemma. Note: In the model card, they wrote “16 devices” but they don’t say what are these devices.

    To run this recipe on consumer hardware, we will use LoRA and quantization, i.e., QLoRA. I’ll detail the LoRA configuration in the next section.

    They used FlahsAttention for faster training on long sequences. We will also use it but note that it is only compatible with recent GPUs (from the Ampere generation, e.g., RTX 30xx or more recent). They also used the bf16 data type. If your GPU is too old to support, you can use float16 but the training might be less stable.

    Other important hyperparameters:

    Learning rate (LR): 2e-5
    LR scheduler: cosine (I don’t know why they chose cosine. They also made this choice for the original Zephyr)
    warmup ratio: 0.1
    training batch size per device: 16
    maximum sequence length: 2048 (If you run out of memory, decrease it. 512 would be the minimum recommended)
    number of training epochs: 3
    DPO Training
    The DPO training follows SFT. The resulting model is the one released by Hugging Face as “Zephyr Gemma 7B”:

    HuggingFaceH4/zephyr-7b-gemma-v0.1
    For DPO training, we need a dataset of preferences pairing prompts with “chosen” and “rejected” outputs generated by LLMs. They used a dataset compiled by Argilla:

    argilla/dpo-mix-7k (MIT license)
    This dataset is a subsample of much larger datasets. Only examples of highly rated “chosen” outputs have been kept.

    Similar to SFT, they provide their recipe and hyperparameters in their alignment handbook. Strangely, their configuration file doesn’t mention FlashAttention. For some reason, they didn’t use it for DPO but we will use it. Note: Anyway, Unsloth exploits FlashAttention by default.

    The most important hyperparameter specific to DPO training is the “beta” which you can see as a regularization parameter. They used the value “0.05”.

    Other important hyperparameters:

    Learning rate (LR): 5e-7 (The same learning rate used by the original Zephyr)
    LR scheduler: cosine
    warmup ratio: 0.1
    training batch size per device: 16
    maximum sequence length: 1024 (It’s lower than for SFT. Maybe the dataset used for DPO training has shorter sequences.)
    number of training epochs: 2
    If the training goes well, it should overfit the training data. This is expected with DPO which has a very weak regularization parameter. This overfitting has been shown beneficial in previous work, so we shouldn’t worry about it.

    IPO, an alternative to DPO, has better regularization. If you want to try it, follow the steps detailed in this article:
    </div>
    <div>
        Finding good training hyperparameters for new LLMs is always difficult and time-consuming. With Zephyr Gemma 7B, Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used a combination of distilled supervised fine-tuning and DPO similar to what they did for their original Zephyr based on Mistral 7B. However, training Gemma with DPO on consumer hardware is challenging due to its memory consumption.

        In this article, I first review the recipe used by Hugging Face to train Zephyr Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing various optimizations for fast and memory-efficient training. The method presented in this article has a peak memory consumption of 19 GB of VRAM and a total training time of only 8 hours. In other words, DPO training for Gemma is possible on consumer hardware.

        A Closer Look at Zephyr Gemma
        Supervised Fine-tuning (SFT)
        DPO must use for reference a model trained with supervised fine-tuning (SFT) on an instruction dataset. Hugging Face also released this SFT model:

        HuggingFaceH4/zephyr-7b-gemma-sft-v0.1
        For SFT, they used deita-10k which is a small instruction dataset of 9.5k examples:

        HuggingFaceH4/deita-10k-v0-sft (MIT license)
        A wide variety of LLMs have generated all the examples in this dataset (GPT-4, GPT-3.5, Claude, Vicuna, Llama 2, Mistral 7B, Zephyr, etc.). For SFT training, they used a special data format that we will also use.

        Hugging Face used the hyperparameters referenced in this configuration file from their alignment handbook. They didn’t use LoRA or quantization. It means that they probably used many A100/H100 GPUs for training Zephyr Gemma. Note: In the model card, they wrote “16 devices” but they don’t say what are these devices.

        To run this recipe on consumer hardware, we will use LoRA and quantization, i.e., QLoRA. I’ll detail the LoRA configuration in the next section.

        They used FlahsAttention for faster training on long sequences. We will also use it but note that it is only compatible with recent GPUs (from the Ampere generation, e.g., RTX 30xx or more recent). They also used the bf16 data type. If your GPU is too old to support, you can use float16 but the training might be less stable.

        Other important hyperparameters:

        Learning rate (LR): 2e-5
        LR scheduler: cosine (I don’t know why they chose cosine. They also made this choice for the original Zephyr)
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 2048 (If you run out of memory, decrease it. 512 would be the minimum recommended)
        number of training epochs: 3
        DPO Training
        The DPO training follows SFT. The resulting model is the one released by Hugging Face as “Zephyr Gemma 7B”:

        HuggingFaceH4/zephyr-7b-gemma-v0.1
        For DPO training, we need a dataset of preferences pairing prompts with “chosen” and “rejected” outputs generated by LLMs. They used a dataset compiled by Argilla:

        argilla/dpo-mix-7k (MIT license)
        This dataset is a subsample of much larger datasets. Only examples of highly rated “chosen” outputs have been kept.

        Similar to SFT, they provide their recipe and hyperparameters in their alignment handbook. Strangely, their configuration file doesn’t mention FlashAttention. For some reason, they didn’t use it for DPO but we will use it. Note: Anyway, Unsloth exploits FlashAttention by default.

        The most important hyperparameter specific to DPO training is the “beta” which you can see as a regularization parameter. They used the value “0.05”.

        Other important hyperparameters:

        Learning rate (LR): 5e-7 (The same learning rate used by the original Zephyr)
        LR scheduler: cosine
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 1024 (It’s lower than for SFT. Maybe the dataset used for DPO training has shorter sequences.)
        number of training epochs: 2
        If the training goes well, it should overfit the training data. This is expected with DPO which has a very weak regularization parameter. This overfitting has been shown beneficial in previous work, so we shouldn’t worry about it.

        IPO, an alternative to DPO, has better regularization. If you want to try it, follow the steps detailed in this article:
    </div>
    <div>
        Finding good training hyperparameters for new LLMs is always difficult and time-consuming. With Zephyr Gemma 7B, Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used a combination of distilled supervised fine-tuning and DPO similar to what they did for their original Zephyr based on Mistral 7B. However, training Gemma with DPO on consumer hardware is challenging due to its memory consumption.

        In this article, I first review the recipe used by Hugging Face to train Zephyr Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing various optimizations for fast and memory-efficient training. The method presented in this article has a peak memory consumption of 19 GB of VRAM and a total training time of only 8 hours. In other words, DPO training for Gemma is possible on consumer hardware.

        A Closer Look at Zephyr Gemma
        Supervised Fine-tuning (SFT)
        DPO must use for reference a model trained with supervised fine-tuning (SFT) on an instruction dataset. Hugging Face also released this SFT model:

        HuggingFaceH4/zephyr-7b-gemma-sft-v0.1
        For SFT, they used deita-10k which is a small instruction dataset of 9.5k examples:

        HuggingFaceH4/deita-10k-v0-sft (MIT license)
        A wide variety of LLMs have generated all the examples in this dataset (GPT-4, GPT-3.5, Claude, Vicuna, Llama 2, Mistral 7B, Zephyr, etc.). For SFT training, they used a special data format that we will also use.

        Hugging Face used the hyperparameters referenced in this configuration file from their alignment handbook. They didn’t use LoRA or quantization. It means that they probably used many A100/H100 GPUs for training Zephyr Gemma. Note: In the model card, they wrote “16 devices” but they don’t say what are these devices.

        To run this recipe on consumer hardware, we will use LoRA and quantization, i.e., QLoRA. I’ll detail the LoRA configuration in the next section.

        They used FlahsAttention for faster training on long sequences. We will also use it but note that it is only compatible with recent GPUs (from the Ampere generation, e.g., RTX 30xx or more recent). They also used the bf16 data type. If your GPU is too old to support, you can use float16 but the training might be less stable.

        Other important hyperparameters:

        Learning rate (LR): 2e-5
        LR scheduler: cosine (I don’t know why they chose cosine. They also made this choice for the original Zephyr)
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 2048 (If you run out of memory, decrease it. 512 would be the minimum recommended)
        number of training epochs: 3
        DPO Training
        The DPO training follows SFT. The resulting model is the one released by Hugging Face as “Zephyr Gemma 7B”:

        HuggingFaceH4/zephyr-7b-gemma-v0.1
        For DPO training, we need a dataset of preferences pairing prompts with “chosen” and “rejected” outputs generated by LLMs. They used a dataset compiled by Argilla:

        argilla/dpo-mix-7k (MIT license)
        This dataset is a subsample of much larger datasets. Only examples of highly rated “chosen” outputs have been kept.

        Similar to SFT, they provide their recipe and hyperparameters in their alignment handbook. Strangely, their configuration file doesn’t mention FlashAttention. For some reason, they didn’t use it for DPO but we will use it. Note: Anyway, Unsloth exploits FlashAttention by default.

        The most important hyperparameter specific to DPO training is the “beta” which you can see as a regularization parameter. They used the value “0.05”.

        Other important hyperparameters:

        Learning rate (LR): 5e-7 (The same learning rate used by the original Zephyr)
        LR scheduler: cosine
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 1024 (It’s lower than for SFT. Maybe the dataset used for DPO training has shorter sequences.)
        number of training epochs: 2
        If the training goes well, it should overfit the training data. This is expected with DPO which has a very weak regularization parameter. This overfitting has been shown beneficial in previous work, so we shouldn’t worry about it.

        IPO, an alternative to DPO, has better regularization. If you want to try it, follow the steps detailed in this article:
    </div>
    <div>
        Finding good training hyperparameters for new LLMs is always difficult and time-consuming. With Zephyr Gemma 7B, Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used a combination of distilled supervised fine-tuning and DPO similar to what they did for their original Zephyr based on Mistral 7B. However, training Gemma with DPO on consumer hardware is challenging due to its memory consumption.

        In this article, I first review the recipe used by Hugging Face to train Zephyr Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing various optimizations for fast and memory-efficient training. The method presented in this article has a peak memory consumption of 19 GB of VRAM and a total training time of only 8 hours. In other words, DPO training for Gemma is possible on consumer hardware.

        A Closer Look at Zephyr Gemma
        Supervised Fine-tuning (SFT)
        DPO must use for reference a model trained with supervised fine-tuning (SFT) on an instruction dataset. Hugging Face also released this SFT model:

        HuggingFaceH4/zephyr-7b-gemma-sft-v0.1
        For SFT, they used deita-10k which is a small instruction dataset of 9.5k examples:

        HuggingFaceH4/deita-10k-v0-sft (MIT license)
        A wide variety of LLMs have generated all the examples in this dataset (GPT-4, GPT-3.5, Claude, Vicuna, Llama 2, Mistral 7B, Zephyr, etc.). For SFT training, they used a special data format that we will also use.

        Hugging Face used the hyperparameters referenced in this configuration file from their alignment handbook. They didn’t use LoRA or quantization. It means that they probably used many A100/H100 GPUs for training Zephyr Gemma. Note: In the model card, they wrote “16 devices” but they don’t say what are these devices.

        To run this recipe on consumer hardware, we will use LoRA and quantization, i.e., QLoRA. I’ll detail the LoRA configuration in the next section.

        They used FlahsAttention for faster training on long sequences. We will also use it but note that it is only compatible with recent GPUs (from the Ampere generation, e.g., RTX 30xx or more recent). They also used the bf16 data type. If your GPU is too old to support, you can use float16 but the training might be less stable.

        Other important hyperparameters:

        Learning rate (LR): 2e-5
        LR scheduler: cosine (I don’t know why they chose cosine. They also made this choice for the original Zephyr)
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 2048 (If you run out of memory, decrease it. 512 would be the minimum recommended)
        number of training epochs: 3
        DPO Training
        The DPO training follows SFT. The resulting model is the one released by Hugging Face as “Zephyr Gemma 7B”:

        HuggingFaceH4/zephyr-7b-gemma-v0.1
        For DPO training, we need a dataset of preferences pairing prompts with “chosen” and “rejected” outputs generated by LLMs. They used a dataset compiled by Argilla:

        argilla/dpo-mix-7k (MIT license)
        This dataset is a subsample of much larger datasets. Only examples of highly rated “chosen” outputs have been kept.

        Similar to SFT, they provide their recipe and hyperparameters in their alignment handbook. Strangely, their configuration file doesn’t mention FlashAttention. For some reason, they didn’t use it for DPO but we will use it. Note: Anyway, Unsloth exploits FlashAttention by default.

        The most important hyperparameter specific to DPO training is the “beta” which you can see as a regularization parameter. They used the value “0.05”.

        Other important hyperparameters:

        Learning rate (LR): 5e-7 (The same learning rate used by the original Zephyr)
        LR scheduler: cosine
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 1024 (It’s lower than for SFT. Maybe the dataset used for DPO training has shorter sequences.)
        number of training epochs: 2
        If the training goes well, it should overfit the training data. This is expected with DPO which has a very weak regularization parameter. This overfitting has been shown beneficial in previous work, so we shouldn’t worry about it.

        IPO, an alternative to DPO, has better regularization. If you want to try it, follow the steps detailed in this article:
    </div>
    <div>
        Finding good training hyperparameters for new LLMs is always difficult and time-consuming. With Zephyr Gemma 7B, Hugging Face seems to have found a good recipe for fine-tuning Gemma. They used a combination of distilled supervised fine-tuning and DPO similar to what they did for their original Zephyr based on Mistral 7B. However, training Gemma with DPO on consumer hardware is challenging due to its memory consumption.

        In this article, I first review the recipe used by Hugging Face to train Zephyr Gemma 7B. Then, I show how to use this recipe with Unsloth, a framework implementing various optimizations for fast and memory-efficient training. The method presented in this article has a peak memory consumption of 19 GB of VRAM and a total training time of only 8 hours. In other words, DPO training for Gemma is possible on consumer hardware.

        A Closer Look at Zephyr Gemma
        Supervised Fine-tuning (SFT)
        DPO must use for reference a model trained with supervised fine-tuning (SFT) on an instruction dataset. Hugging Face also released this SFT model:

        HuggingFaceH4/zephyr-7b-gemma-sft-v0.1
        For SFT, they used deita-10k which is a small instruction dataset of 9.5k examples:

        HuggingFaceH4/deita-10k-v0-sft (MIT license)
        A wide variety of LLMs have generated all the examples in this dataset (GPT-4, GPT-3.5, Claude, Vicuna, Llama 2, Mistral 7B, Zephyr, etc.). For SFT training, they used a special data format that we will also use.

        Hugging Face used the hyperparameters referenced in this configuration file from their alignment handbook. They didn’t use LoRA or quantization. It means that they probably used many A100/H100 GPUs for training Zephyr Gemma. Note: In the model card, they wrote “16 devices” but they don’t say what are these devices.

        To run this recipe on consumer hardware, we will use LoRA and quantization, i.e., QLoRA. I’ll detail the LoRA configuration in the next section.

        They used FlahsAttention for faster training on long sequences. We will also use it but note that it is only compatible with recent GPUs (from the Ampere generation, e.g., RTX 30xx or more recent). They also used the bf16 data type. If your GPU is too old to support, you can use float16 but the training might be less stable.

        Other important hyperparameters:

        Learning rate (LR): 2e-5
        LR scheduler: cosine (I don’t know why they chose cosine. They also made this choice for the original Zephyr)
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 2048 (If you run out of memory, decrease it. 512 would be the minimum recommended)
        number of training epochs: 3
        DPO Training
        The DPO training follows SFT. The resulting model is the one released by Hugging Face as “Zephyr Gemma 7B”:

        HuggingFaceH4/zephyr-7b-gemma-v0.1
        For DPO training, we need a dataset of preferences pairing prompts with “chosen” and “rejected” outputs generated by LLMs. They used a dataset compiled by Argilla:

        argilla/dpo-mix-7k (MIT license)
        This dataset is a subsample of much larger datasets. Only examples of highly rated “chosen” outputs have been kept.

        Similar to SFT, they provide their recipe and hyperparameters in their alignment handbook. Strangely, their configuration file doesn’t mention FlashAttention. For some reason, they didn’t use it for DPO but we will use it. Note: Anyway, Unsloth exploits FlashAttention by default.

        The most important hyperparameter specific to DPO training is the “beta” which you can see as a regularization parameter. They used the value “0.05”.

        Other important hyperparameters:

        Learning rate (LR): 5e-7 (The same learning rate used by the original Zephyr)
        LR scheduler: cosine
        warmup ratio: 0.1
        training batch size per device: 16
        maximum sequence length: 1024 (It’s lower than for SFT. Maybe the dataset used for DPO training has shorter sequences.)
        number of training epochs: 2
        If the training goes well, it should overfit the training data. This is expected with DPO which has a very weak regularization parameter. This overfitting has been shown beneficial in previous work, so we shouldn’t worry about it.

        IPO, an alternative to DPO, has better regularization. If you want to try it, follow the steps detailed in this article:
    </div>
</div>
{{--    <x-form id="entryForm" title="Translations" center="true">--}}
{{--        <x-slot:fields>--}}
{{--            @foreach($languages as $language)--}}
{{--                @php--}}
{{--                    $idLanguage = $language['idLanguage'];--}}
{{--                    $description = mb_ereg_replace("\r\n","\\n",$entries[$idLanguage]['description']);--}}
{{--                @endphp--}}
{{--                <x-card--}}
{{--                    title="{{$language['description']}}"--}}
{{--                    class="mb-4"--}}
{{--                >--}}
{{--                    <x-hidden-field--}}
{{--                        id="idEntry_{{$idLanguage}}"--}}
{{--                        :value="$entries[$idLanguage]['idEntry']"--}}
{{--                    ></x-hidden-field>--}}
{{--                    <x-text-field--}}
{{--                        label="Name"--}}
{{--                        id="name_{{$idLanguage}}"--}}
{{--                        :value="$entries[$idLanguage]['name']"--}}
{{--                    ></x-text-field>--}}
{{--                    <x-multiline-field--}}
{{--                        label="Definition"--}}
{{--                        id="description_{{$idLanguage}}"--}}
{{--                        value="{{$description}}"--}}
{{--                    >--}}
{{--                    </x-multiline-field>--}}
{{--                </x-card>--}}
{{--            @endforeach--}}
{{--        </x-slot:fields>--}}
{{--        <x-slot:buttons>--}}
{{--            <x-submit label="Save" hx-put="/entry"></x-submit>--}}
{{--        </x-slot:buttons>--}}
{{--    </x-form>--}}
